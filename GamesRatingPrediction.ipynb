{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c537d8",
   "metadata": {},
   "source": [
    "# Game Rating Prediction\n",
    "## Based off ML, Kafka, Spark\n",
    "### By: Natty Zepko (302309513) , Amit Yehezkel (211899489)\n",
    "\n",
    "In the following project we will be using MLto predict the rating of games in the Steam store.\n",
    "Each game has the following features:\n",
    "\n",
    "| Feature | Type | Description |\n",
    "| :-- | :-- | :-- |\n",
    "| App ID | INT | Uniquely assigned ID number given by Steam |\n",
    "| Name | STRING (utf-8) | The full name of the product. May include special characters, emojis, and characters from other languages |\n",
    "| Total Reviews | INT | The total amount of reviews the product has on steam (both negative and positive) |\n",
    "| Positive Reviews Percent | DOUBLE | number of positive reviews divided by the total reviews |\n",
    "| Developer | STRING (utf-8) | The credited Developer. May include special characters, emojis, and characters from other languages |\n",
    "| Publisher | STRING (utf-8) | The credited Publisher. May include special characters, emojis, and characters from other languages |\n",
    "| Metacritic Score | INT | Score ranged from 0 to 100 (inclusive), represents an internal weighted average of professional raters. Provided by steam, or set to 50 in case the product isn't rated. |\n",
    "| Is Free | 0/1 | 1 if free, 0 otherwise |\n",
    "| Genre 1 | INT | The first, most defining Genre of the game. -1 if none provided |\n",
    "| Genre 2 | INT | The secondary defined Genre of the game. -1 if none provided |\n",
    "| Genre 3 | INT | The thirdly defining Genre of the game. -1 if none provided |\n",
    "| Peak | INT | The largest amount of players playing the game at the same time (since launch) |\n",
    "\n",
    "\n",
    "Peak, specifically was not provided by steam, but was collected from steam-charts. It would default to -1 if not found on their website, despite out expectation to not find any, due to us filtering the games with less than 50 reviews (which are often written after playing the game for some time, so value of at least 1 is expected for all games).\n",
    "\n",
    "We were, however, proven wrong, as we found games and products that have 0 players because it is either a DLC (Downloadable Content, meant to be added to an existing game), or test products that can't be played, or used. There products have 0 \"Peak\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8236ba3f",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68362a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# main packages\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as func\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# functions and tools\n",
    "from time import sleep\n",
    "from threading import Thread\n",
    "from functools import partial\n",
    "\n",
    "# pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, ClusteringEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, PCA, StandardScaler, StopWordsRemover, CountVectorizer\n",
    "\n",
    "# Kafka\n",
    "from confluent_kafka import Producer, Consumer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed7062",
   "metadata": {},
   "source": [
    "# First step: Reading the CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56f683fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[appid: int, name: string, total_reviews: int, positive_reviews_percent: double, developer: string, publisher: string, metacritic_score: int, is_free: int, genre1: int, genre2: int, genre3: int, price: double, peak: int]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"SteamPile\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"steam_app_list_detailed_v2.csv\", header=True, inferSchema=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354128f7",
   "metadata": {},
   "source": [
    "# Second step: Feature Engineering:\n",
    "\n",
    "We need to assort each developer and publisher into categories. This can be done through indexing, such that we replace names with integers. This makes the ML easier to recognize a pattern (rather than analyze characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eef1e0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/31 19:33:45 WARN DAGScheduler: Broadcasting large task binary with size 1184.8 KiB\n",
      "+-------------+------------------------+----------------+------+------+------+-------+----+--------------+--------------+\n",
      "|total_reviews|positive_reviews_percent|metacritic_score|genre1|genre2|genre3|  price|peak|developerIndex|publisherIndex|\n",
      "+-------------+------------------------+----------------+------+------+------+-------+----+--------------+--------------+\n",
      "|          105|      0.8761904761904762|              50|     1|    25|    23|  499.0|  25|         12137|          9473|\n",
      "|          106|      0.7075471698113207|              50|     1|     4|     9| 3695.0|   6|          7883|          6099|\n",
      "|           59|       0.711864406779661|              50|     1|    25|    37|20995.0|   0|           184|           328|\n",
      "|           74|      0.4864864864864865|              50|    25|    23|     3| 1850.0|   0|          4322|           475|\n",
      "|          139|      0.5899280575539568|              50|     4|    23|    -1| 3695.0| 254|           131|           205|\n",
      "|           68|      0.3676470588235294|              50|    25|    23|    70| 1450.0| 112|         15006|         11675|\n",
      "|          478|      0.7677824267782427|              50|     4|    28|    -1| 2595.0| 150|          6967|          5397|\n",
      "|          149|      0.9463087248322148|              50|     1|    25|     4|    0.0|  46|           129|           217|\n",
      "|        16675|      0.9805697151424289|              50|     4|    37|    23|    0.0| 419|           454|           523|\n",
      "|         2680|      0.6582089552238806|              50|     1|    25|     4| 1850.0|1613|           231|           333|\n",
      "|           58|      0.1206896551724138|              50|     1|    37|    -1| 1200.0|   0|           418|           246|\n",
      "|           85|                     0.2|              50|     1|    37|    -1|    0.0|   0|           418|           246|\n",
      "|           58|      0.4482758620689655|              50|     1|    37|    -1| 9900.0|   0|           418|           246|\n",
      "|           75|      0.9466666666666668|              50|    25|    -1|    -1| 4165.0|  17|          2589|           151|\n",
      "|           57|      0.8947368421052632|              50|    25|     4|    37|    0.0|   7|          6538|          1868|\n",
      "|           68|      0.8823529411764706|              50|     1|     4|    37|    0.0|  18|          5190|          4065|\n",
      "|           60|      0.8666666666666667|              50|     9|    28|    18|    0.0|  54|           736|           611|\n",
      "|           91|      0.8351648351648352|              50|     2|    -1|    -1|    0.0|   0|           149|             1|\n",
      "|           64|                     1.0|              50|     4|    -1|    -1| 1095.0|  28|           786|           786|\n",
      "|          208|      0.8942307692307693|              50|     1|    23|     3| 9295.0| 149|           310|           443|\n",
      "+-------------+------------------------+----------------+------+------+------+-------+----+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fix_dataframe(df):\n",
    "    #Remove \"is free\"\n",
    "    df=df.drop(\"is_free\")\n",
    "\n",
    "    #Change developer name into categorical numbers\n",
    "    indexer = StringIndexer(inputCol=\"developer\", outputCol=\"developerIndex\").setHandleInvalid(\"keep\")\n",
    "    indexed = indexer.fit(df).transform(df)\n",
    "    df = indexed.drop(\"developer\")\n",
    "    df = df.withColumn(\"developerIndex\", func.round(df[\"developerIndex\"]).cast('integer'))\n",
    "\n",
    "    #Change publisher name into categorical numbers\n",
    "    indexer = StringIndexer(inputCol=\"publisher\", outputCol=\"publisherIndex\").setHandleInvalid(\"keep\")\n",
    "    indexed = indexer.fit(df).transform(df)\n",
    "    df = indexed.drop(\"publisher\")\n",
    "    df = df.withColumn(\"publisherIndex\", func.round(df[\"publisherIndex\"]).cast('integer'))\n",
    "\n",
    "    #remove name and ID from model (as it isn't neccesary for ML model)\n",
    "    df2=df.drop(\"name\")\n",
    "    df2=df2.drop(\"appid\")\n",
    "    return df2\n",
    "df2 = fix_dataframe(df)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7894d69",
   "metadata": {},
   "source": [
    "# Third step: Turn DF into vector:\n",
    "\n",
    "We write the functions for transforming a DF into a vector used for ML. We add an option to do it with, or without labels, for training purposed, or for prediction without labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81e3e963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/31 19:33:45 WARN DAGScheduler: Broadcasting large task binary with size 1197.0 KiB\n",
      "+--------------------+------------------+\n",
      "|            features|             label|\n",
      "+--------------------+------------------+\n",
      "|[50.0,1.0,25.0,23...|0.8761904761904762|\n",
      "|[50.0,1.0,4.0,9.0...|0.7075471698113207|\n",
      "|[50.0,1.0,25.0,37...| 0.711864406779661|\n",
      "|[50.0,25.0,23.0,3...|0.4864864864864865|\n",
      "|[50.0,4.0,23.0,-1...|0.5899280575539568|\n",
      "|[50.0,25.0,23.0,7...|0.3676470588235294|\n",
      "|[50.0,4.0,28.0,-1...|0.7677824267782427|\n",
      "|[50.0,1.0,25.0,4....|0.9463087248322148|\n",
      "|[50.0,4.0,37.0,23...|0.9805697151424289|\n",
      "|[50.0,1.0,25.0,4....|0.6582089552238806|\n",
      "|[50.0,1.0,37.0,-1...|0.1206896551724138|\n",
      "|[50.0,1.0,37.0,-1...|               0.2|\n",
      "|[50.0,1.0,37.0,-1...|0.4482758620689655|\n",
      "|[50.0,25.0,-1.0,-...|0.9466666666666668|\n",
      "|[50.0,25.0,4.0,37...|0.8947368421052632|\n",
      "|[50.0,1.0,4.0,37....|0.8823529411764706|\n",
      "|[50.0,9.0,28.0,18...|0.8666666666666667|\n",
      "|[50.0,2.0,-1.0,-1...|0.8351648351648352|\n",
      "|[50.0,4.0,-1.0,-1...|               1.0|\n",
      "|[50.0,1.0,23.0,3....|0.8942307692307693|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transform_vector(df2):\n",
    "    # Tranforming features to vectors\n",
    "    chosen_columns = [\"metacritic_score\", \"genre1\", \"genre2\", \"genre3\",\"price\",\"peak\",\"developerIndex\",\"publisherIndex\"]\n",
    "    vectors_assembler = VectorAssembler(inputCols=chosen_columns, outputCol=\"features\")\n",
    "    df3= vectors_assembler.transform(df2).select(col(\"features\"),col(\"positive_reviews_percent\"))\n",
    "    df3=df3.withColumnRenamed(\"positive_reviews_percent\", \"label\")\n",
    "    return df3\n",
    "\n",
    "def trasnform_vector_without_label(df2):\n",
    "    # Tranforming features to vectors\n",
    "    chosen_columns = [\"metacritic_score\", \"genre1\", \"genre2\", \"genre3\",\"price\",\"peak\",\"developerIndex\",\"publisherIndex\"]\n",
    "    vectors_assembler = VectorAssembler(inputCols=chosen_columns, outputCol=\"features\")\n",
    "    df3= vectors_assembler.transform(df2).select(\"features\")\n",
    "    return df3\n",
    "df3 = transform_vector(df2)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfef262",
   "metadata": {},
   "source": [
    "# Fourth step: Model building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8194d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df):\n",
    "    # Splits the dataframe to train (80%) and validation (20%)\n",
    "    train, validation = df.randomSplit([0.8, 0.2], seed = 812)\n",
    "    # Trains a classification model on the train set and makes predictions on the validation set\n",
    "    model = LinearRegression(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"predicted_medv\").fit(train)\n",
    "    predictions = model.transform(validation)\n",
    "    \n",
    "    evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"predicted_medv\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(\"Root Mean Squared Error (RMSE) on test data: {:.3f}\".format(rmse))\n",
    "    return model\n",
    "\n",
    "def predict(model, df):\n",
    "    df = fix_dataframe(df)\n",
    "    df = trasnform_vector_without_label(df)\n",
    "    return model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ca694",
   "metadata": {},
   "source": [
    "## Now we just have to apply the training function to the dataframe:\n",
    "\n",
    "We would require the RMSE value to be less than 0.2 at least, to know if our model's average error is as low as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b896b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/31 19:33:45 WARN Instrumentation: [a666ff8d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/07/31 19:33:45 WARN DAGScheduler: Broadcasting large task binary with size 1356.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 59:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/31 19:33:46 WARN DAGScheduler: Broadcasting large task binary with size 1362.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 60:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/31 19:33:47 WARN DAGScheduler: Broadcasting large task binary with size 1363.0 KiB\n",
      "Root Mean Squared Error (RMSE) on test data: 0.162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_85616399a27f, numFeatures=8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabaaa5",
   "metadata": {},
   "source": [
    "### Feature testing allowed us to discover that \"Total Reviews\" is redundant. We removed it, and got the same RMSE score (or slightly lower, by less that 1% with a different random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "738adc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/31 19:33:48 WARN Instrumentation: [180ed568] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/07/31 19:33:48 WARN DAGScheduler: Broadcasting large task binary with size 1356.7 KiB\n",
      "23/07/31 19:33:48 WARN DAGScheduler: Broadcasting large task binary with size 1362.5 KiB\n",
      "23/07/31 19:33:49 WARN DAGScheduler: Broadcasting large task binary with size 1363.0 KiB\n",
      "Root Mean Squared Error (RMSE) on test data: 0.162\n"
     ]
    }
   ],
   "source": [
    "# Tranforming features to vectors\n",
    "chosen_columns = [\"metacritic_score\", \"genre1\", \"genre2\", \"genre3\",\"price\",\"peak\",\"developerIndex\",\"publisherIndex\"]\n",
    "vectors_assembler = VectorAssembler(inputCols=chosen_columns, outputCol=\"features\")\n",
    "vectors_assembler\n",
    "\n",
    "df3=vectors_assembler.transform(df2).select(col(\"features\"),col(\"positive_reviews_percent\"))\n",
    "df3=df3.withColumnRenamed(\"positive_reviews_percent\", \"label\")\n",
    "games_model = train(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec135cb",
   "metadata": {},
   "source": [
    "### The RMSE result is about 0.162, lower than 0.2, as we required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265213a1",
   "metadata": {},
   "source": [
    "# Fifth Step: Kafka (sorting the data into 'topics'):\n",
    "\n",
    "We split the data into 10 different CSV files, each containing aprox. 3,000 lines of games data.\n",
    "\n",
    "Our initial idea was letting the games be sorted into 5 topics, based on P, the predicted percent of positive reviews our model would predict:\n",
    "\n",
    "| Topic | classification |\n",
    "| :-- | :-: |\n",
    "| Gold | 0.9 < P |\n",
    "| Positive | 0.7 < P <= 0.9 |\n",
    "| Mixed | 0.4 < P <= 0.7 |\n",
    "| Negative | 0.2 < P <= 0.4 |\n",
    "| Trash | 0 <= P <= 0.2 |\n",
    "\n",
    "While planning, our thought proccess was that on average we would expect the five categories to be equally populated, or somewhat spread through the top three categories the most, while somewhat rare in the bottom two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "466dc859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1690821230.093|CONFWARN|rdkafka#producer-31| [thrd:app]: Configuration property group.id is a consumer property and will be ignored by this producer instance\n",
      "%4|1690821230.093|CONFWARN|rdkafka#producer-31| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n",
      "%4|1690821230.094|CONFWARN|rdkafka#producer-32| [thrd:app]: Configuration property group.id is a consumer property and will be ignored by this producer instance\n",
      "%4|1690821230.094|CONFWARN|rdkafka#producer-32| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n",
      "%4|1690821230.094|CONFWARN|rdkafka#producer-33| [thrd:app]: Configuration property group.id is a consumer property and will be ignored by this producer instance\n",
      "%4|1690821230.094|CONFWARN|rdkafka#producer-33| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n",
      "%4|1690821230.095|CONFWARN|rdkafka#producer-34| [thrd:app]: Configuration property group.id is a consumer property and will be ignored by this producer instance\n",
      "%4|1690821230.095|CONFWARN|rdkafka#producer-34| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n",
      "%4|1690821230.095|CONFWARN|rdkafka#producer-35| [thrd:app]: Configuration property group.id is a consumer property and will be ignored by this producer instance\n",
      "%4|1690821230.095|CONFWARN|rdkafka#producer-35| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n"
     ]
    }
   ],
   "source": [
    "TOPICS = [\"Gold\", \"Positive\", \"Mixed\", \"Negative\", \"Trash\"]\n",
    "CONFIGURATIONS = {\"bootstrap.servers\": \"localhost:9092\", \"group.id\": \"game\", \"auto.offset.reset\": \"smallest\"}\n",
    "PRODUCERS = dict(zip(TOPICS, [Producer(CONFIGURATIONS) for topic in TOPICS]))\n",
    "CONSUMERS = dict(zip(TOPICS, [Consumer(CONFIGURATIONS) for topic in TOPICS]))\n",
    "\n",
    "\n",
    "def game_rank_by_percent(percent):\n",
    "    if percent > 0.9:\n",
    "        return TOPICS[0]\n",
    "    if percent > 0.7:\n",
    "        return TOPICS[1]\n",
    "    if percent > 0.4:\n",
    "        return TOPICS[2]\n",
    "    if percent > 0.2:\n",
    "        return TOPICS[3]\n",
    "    return TOPICS[4]\n",
    "\n",
    "def produce_games(batch_df, batch_id):\n",
    "    # Processes and classifies the jobs batch \n",
    "    games_df = predict(games_model, batch_df.select(\"metacritic_score\", \"genre1\", \"genre2\", \"genre3\",\"price\",\"peak\",\"developer\",\"publisher\"))\n",
    "    a = games_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "    b = batch_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "    games_df = a.join(b, a.row_idx == b.row_idx).drop(\"row_idx\")\n",
    "\n",
    "    # Produces the classified jobs batch to topics\n",
    "    games_df = games_df.rdd.map(lambda x: (x.name, game_rank_by_percent(x.predicted_medv), x.predicted_medv))\n",
    "    games_df = games_df.toDF([\"name\", \"rank\", \"prediction_score\"])\n",
    "    for game in games_df.rdd.collect():\n",
    "        gameRank = game[\"rank\"]\n",
    "        message = \"The game \" + game[\"name\"] +\" Has the rating of \" + str(game[\"prediction_score\"]) + \"\\n\"\n",
    "        \n",
    "        PRODUCERS[gameRank].produce(gameRank, value=message)\n",
    "        PRODUCERS[gameRank].flush()  \n",
    "        \n",
    "        sleep(0.2)\n",
    "\n",
    "def consume_games(consumer, topic):\n",
    "    try:      \n",
    "        while consumers_active:\n",
    "            # Polls for new messages\n",
    "            message = consumer.poll(timeout=3.0)\n",
    "            \n",
    "            if message is not None:\n",
    "                # Saves the message to file\n",
    "                message = message.value().decode(\"utf-8\")\n",
    "                with open(topic +\".txt\", \"a\") as file:\n",
    "                    file.write(message)\n",
    "                \n",
    "            #sleep(0.1)\n",
    "            \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        \n",
    "    finally:\n",
    "        # Closes the consumer to commit final offsets\n",
    "        consumer.close()\n",
    "\n",
    "# Initializes the consumers\n",
    "consumers_active = True\n",
    "consumer_threads = []\n",
    "\n",
    "for topic, consumer in CONSUMERS.items():\n",
    "    # Subscribes the consumer to its topic\n",
    "    consumer.subscribe([topic])\n",
    "    # Creates thread for each consumer\n",
    "    thread = Thread(target=consume_games, args=(consumer, topic))\n",
    "    consumer_threads.append(thread)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d6fc9",
   "metadata": {},
   "source": [
    "# Last step: distributing the data among the subscribers.\n",
    "\n",
    "We run the kafka.sh file in the terminal prior to reaching this stage.\n",
    "After processing all 10 files, we stop the runtime manually (therefore we expect a runtime error at the bottom of this code snippet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b43471b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/31 19:33:50 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-30b13787-0b99-4854-8f71-f4d1b44b3411. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/07/31 19:33:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/07/31 19:33:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/07/31 19:33:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/07/31 19:33:51 WARN DAGScheduler: Broadcasting large task binary with size 1216.4 KiB\n",
      "23/07/31 19:33:52 WARN DAGScheduler: Broadcasting large task binary with size 1383.5 KiB\n",
      "23/07/31 19:33:52 WARN DAGScheduler: Broadcasting large task binary with size 1389.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mreadStream\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mschema(df\u001b[38;5;241m.\u001b[39mschema)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./CSV\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Starts the streaming\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduce_games\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ex)\n",
      "File \u001b[0;32m/usr/local/spark/spark/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Streams CSV files into a streaming DataFrame\n",
    "    test_df = spark.readStream.format(\"csv\").option(\"header\", \"true\").schema(df.schema).load(\"./CSV\")\n",
    "\n",
    "    # Starts the streaming\n",
    "    test_df.writeStream.foreachBatch(partial(produce_games)).start().awaitTermination()\n",
    "    \n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    consumers_active = False\n",
    "    for thread in consumer_threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef704bc",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "The model did not behave as we expected.\n",
    "It isn't a bad thing, however, as our expectations, and the data we analyzed is just that homogenous.\n",
    "\n",
    "| Topic | classification | number of results |\n",
    "| :-- | :-: | :-: |\n",
    "| Gold | 0.9 < P | 8 |\n",
    "| Positive | 0.7 < P <= 0.9 | 30,000 |\n",
    "| Mixed | 0.4 < P <= 0.7 | 1 |\n",
    "| Negative | 0.2 < P <= 0.4 | 0 |\n",
    "| Trash | 0 <= P <= 0.2 | 0 |\n",
    "\n",
    "The games were not divided equally between the categories. Most games were given scores between 0.7 and 0.9.\n",
    "It is, however, somewhat expected. Most games on steam with at least 50 reviews are often scored in that range because of the nature of reviewing games on steam.\n",
    "We did not take into account the spread of scores, and had we anticipated the division of scores better, we would catagorize the topics in a slightly different configurations, for example, we propose:\n",
    "\n",
    "| Topic | classification |\n",
    "| :-- | :-: |\n",
    "| Best | 0.9 < P |\n",
    "| Fantastic | 0.85 < P <= 0.9 |\n",
    "| Highly Recommended | 0.8 < P <= 0.85 |\n",
    "| Recommended | 0.78 < P <= 0.8 |\n",
    "| Slightly Recommended | 0.76 < P <= 0.78 |\n",
    "| Well Recieved | 0.72 < P <= 0.76 |\n",
    "| Average | 0.7 < P <= 0.72 |\n",
    "| Badly Recieved | 0 <= P <= 0.7 |\n",
    "\n",
    "The re-categorization and re-running can be done in aprox. 2 hours runtime, though we do not feel it is neccessary to apply these changes on our existing data. It can be done on future games, for a good prediction model.\n",
    "\n",
    "## Why are all the games in that range?\n",
    "\n",
    "Our guess is that the model we rewarded for guessing numbers in that range for the sheer amount of examples in that score range. The average error is evidently quite little considering most guesses are in a good ballpark distance from the actual score.\n",
    "\n",
    "The model did not have to get the \"bad games\" right, it just had to get all of the \"average games\" right, for their sheer quantity. Most games just don't recieve that bad of a score, in real life, and due to how popularity works, the ones with low scores get few reviews and thus don't end up in our model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
